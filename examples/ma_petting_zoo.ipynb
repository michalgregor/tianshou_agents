{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install gym[classic_control]\n",
    "!{sys.executable} -m pip install tianshou\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/tianshou_agents.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "from tianshou_agents.methods.ma import ma_default\n",
    "from tianshou_agents.methods.dqn import dqn_default\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from tianshou.trainer import OffpolicyTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Multi-Agent Policy with PettingZoo Envs\n",
    "\n",
    "This notebook gives a brief illustration of how to use the multi-agent policy in Tianshou Agents together with a PettingZoo env. The hyperparameters are from the [Tic-Tac-Toe example in Tianshou](https://github.com/thu-ml/tianshou/blob/master/test/pettingzoo/tic_tac_toe.py).\n",
    "\n",
    "First of all, we are going to set up a function that constructs our PettingZoo environment and wraps it in Tianshou's ``PettingZooEnv``. We also define a custom function that will extract the observation shape from the observation space. The observation space here is a bit special as Gym environments go – it is a dictionary space and the actual observations live under the `'observation'` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\michal\\work\\tianshou_agents\\tianshou_agents\\networks.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "def get_env():\n",
    "    return PettingZooEnv(tictactoe_v3.env())\n",
    "\n",
    "def extract_obs_shape(observation_space):\n",
    "    return observation_space['observation'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a configuration for our policies, we are going to retrieve a config from the ``dqn_default`` preset. Then we are going to modify the ``'hidden_sizes'`` of the ``'qnetwork'``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_config = dqn_default.derive_conf()['policy']\n",
    "policy_config['qnetwork']['hidden_sizes'] = [128, 128, 128, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having prepared a policy config, we now have several options. We could, for instance, just pass ``policies=[policy_config, policy_config]`` to the ``ma_default`` preset and that would automatically construct two policies with the same configuration.\n",
    "\n",
    "What we can also do, is define a custom function, that is going to return a list of already built policies. We are going to take this latter approach here, because it is a bit more flexible. We can, for instance, create just a single policy instance, and use it to control both agents. This is not necessarily going to help the learning process, but it serves to illustrate the kind of modeling freedom that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_policy(agent, device, seed, **kwargs):\n",
    "    policy = agent.config_router.policy_builder(\n",
    "        config=policy_config,\n",
    "        default_kwargs=dict(kwargs,\n",
    "            agent=agent,\n",
    "            device=device,\n",
    "            seed=seed\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return [policy, policy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having figured out how to construct the policies, we can now use the ``ma_default`` preset to construct our agent. The interface is as usual – we can call ``agent.train()`` to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [01:16, 13.13it/s, env_step=1000, len=5, n/ep=0, n/st=1, player_1/loss=0.121, player_2/loss=0.130, rew=0.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 0.000000 ± 1.000000, best_reward: 0.000000 ± 1.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [01:45,  9.47it/s, env_step=2000, len=6, n/ep=0, n/st=1, player_1/loss=0.062, player_2/loss=0.080, rew=0.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 0.000000 ± 1.000000, best_reward: 0.000000 ± 1.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 1001it [01:21, 12.27it/s, env_step=3000, len=9, n/ep=0, n/st=1, player_1/loss=0.033, player_2/loss=0.058, rew=0.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 0.000000 ± 1.000000, best_reward: 0.000000 ± 1.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 1001it [01:08, 14.57it/s, env_step=4000, len=7, n/ep=0, n/st=1, player_1/loss=2.419, player_2/loss=0.080, rew=0.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 0.000000 ± 1.000000, best_reward: 0.000000 ± 1.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 1001it [00:59, 16.92it/s, env_step=5000, len=8, n/ep=0, n/st=1, player_1/loss=0.031, player_2/loss=0.048, rew=0.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 0.000000 ± 1.000000, best_reward: 0.000000 ± 1.000000 in #0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'duration': '392.33s',\n",
       " 'train_time/model': '356.88s',\n",
       " 'test_step': 38,\n",
       " 'test_episode': 6,\n",
       " 'test_time': '0.18s',\n",
       " 'test_speed': '215.91 step/s',\n",
       " 'best_reward': 0.0,\n",
       " 'best_result': '0.00 ± 1.00',\n",
       " 'train_step': 5000,\n",
       " 'train_episode': 669,\n",
       " 'train_time/collector': '35.27s',\n",
       " 'train_speed': '12.75 step/s'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = ma_default(\n",
    "    'TicTacToe',\n",
    "    task=get_env,\n",
    "    trainer_class=OffpolicyTrainer,\n",
    "    policies=make_policy,\n",
    "    replay_buffer=20000,\n",
    "    max_epoch=5,\n",
    "    step_per_epoch=1000,\n",
    "    extract_obs_shape=extract_obs_shape\n",
    ")\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the agent, we can call ``agent.test`` just like we would do is a regular single-agent setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 60,\n",
       " 'rews': array([[-1.,  1.],\n",
       "        [-1.,  1.],\n",
       "        [-1.,  1.],\n",
       "        [-1.,  1.],\n",
       "        [-1.,  1.],\n",
       "        [-1.,  1.],\n",
       "        [-1.,  1.],\n",
       "        [-1.,  1.],\n",
       "        [-1.,  1.],\n",
       "        [-1.,  1.]]),\n",
       " 'lens': array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]),\n",
       " 'idxs': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'rew': 0.0,\n",
       " 'len': 6.0,\n",
       " 'rew_std': 1.0,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.test(episode_per_test=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a351393365bb1b108989afa08de3243f72f5e58927baf5d192f3cca79a41cbc4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
