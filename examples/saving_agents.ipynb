{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "import shutil\n",
    "USE_NBCAP = False\n",
    "\n",
    "if not shutil.which('apt') is None:\n",
    "    !apt update\n",
    "    !apt install -y xvfb x11-utils\n",
    "    !{sys.executable} -m pip install pyscreenshot pyvirtualdisplay\n",
    "    !{sys.executable} -m pip install --upgrade pyglet\n",
    "    !{sys.executable} -m pip install git+https://github.com/michalgregor/nbcap.git\n",
    "\n",
    "    USE_NBCAP = True\n",
    "\n",
    "!{sys.executable} -m pip install gym[classic_control]\n",
    "!{sys.executable} -m pip install class_utils[tensorboard]@git+https://github.com/michalgregor/class_utils.git\n",
    "!{sys.executable} -m pip install tianshou\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/tianshou_agents.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "%load_ext tensorboard\n",
    "\n",
    "import shutil\n",
    "if shutil.which('apt') is None:\n",
    "    USE_NBCAP = False\n",
    "else:\n",
    "    USE_NBCAP = True\n",
    "\n",
    "    from nbcap import ShowVideoCallback, ScreenRecorder, OutputManager, DisplayProcess\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tianshou_agents.utils import VectorEnvRenderWrapper\n",
    "from tianshou_agents.sac import sac_simple\n",
    "from tianshou.data import Collector\n",
    "from tianshou.env import BaseVectorEnv\n",
    "from tianshou_agents.preset import AgentPresetWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "\n",
    "if USE_NBCAP:\n",
    "    display_size=(600, 400)\n",
    "    show_video = ShowVideoCallback(dimensions=display_size)\n",
    "\n",
    "    # make sure that only one instance\n",
    "    # of the display is ever created\n",
    "    try:\n",
    "        DISP_PROC\n",
    "    except NameError:\n",
    "        DISP_PROC = DisplayProcess(display_size=display_size)\n",
    "\n",
    "    def make_screen_recorder(max_gui_outputs=1):\n",
    "        video_path=\"output\"\n",
    "        segment_time=10\n",
    "\n",
    "        output_manager = OutputManager(max_gui_outputs=max_gui_outputs)\n",
    "        video_callback=output_manager(show_video)\n",
    "        display = DISP_PROC.id\n",
    "\n",
    "        screen_recorder = ScreenRecorder(\n",
    "            display, display_size, video_path,\n",
    "            segment_time=segment_time, video_callback=video_callback\n",
    "        )\n",
    "        \n",
    "        return screen_recorder\n",
    "\n",
    "    SCREEN_RECORDER = make_screen_recorder()\n",
    "else:\n",
    "    from contextlib import suppress\n",
    "    SCREEN_RECORDER = suppress()\n",
    "\n",
    "class RenderCollector(Collector):\n",
    "    def __init__(self, collector, render=0.01):\n",
    "        self.collector = collector\n",
    "\n",
    "        if isinstance(self.collector.env, BaseVectorEnv):\n",
    "            self.collector.env = VectorEnvRenderWrapper(\n",
    "                self.collector.env)\n",
    "\n",
    "        self._render = render\n",
    "\n",
    "    @property\n",
    "    def collect_time(self):\n",
    "        return max(self.collector.collect_time, 1e-20)\n",
    "\n",
    "    @collect_time.setter\n",
    "    def collect_time(self, val):\n",
    "        self.collector.collect_time = val\n",
    "\n",
    "    def collect(\n",
    "        self, n_step = None, n_episode = None, random = False,\n",
    "        render = None, no_grad = True,\n",
    "    ):\n",
    "        with SCREEN_RECORDER:\n",
    "            render = render or self._render\n",
    "            return self.collector.collect(n_step, n_episode, random, render, no_grad)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name.startswith('_'):\n",
    "            raise AttributeError(\"attempted to get missing private attribute '{}'\".format(name))\n",
    "        return getattr(self.collector, name)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{}{}>'.format(type(self).__name__, self.collector)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "class AgentPresetPatch(AgentPresetWrapper):\n",
    "    def __init__(self, preset, render=0.01):\n",
    "        super().__init__(preset)\n",
    "        self._prev_test_envs = None\n",
    "        self.render = render\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        agent = self._preset(*args, **kwargs)\n",
    "        \n",
    "        # we close the previous pyglet window before\n",
    "        # opening a new one to work around a bug on Windows\n",
    "        if not self._prev_test_envs is None:\n",
    "            self._prev_test_envs.close()\n",
    "\n",
    "        agent.test_collector = RenderCollector(\n",
    "            agent.test_collector, render=self.render\n",
    "        )\n",
    "        \n",
    "        self._prev_test_envs = agent.test_envs\n",
    "        \n",
    "        return agent\n",
    "        \n",
    "sac_simple = AgentPresetPatch(sac_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing and Saving Agents\n",
    "\n",
    "This example illustrates how agents are checkpointed automatically and how the saving and loading of agent's state works. This is not only useful when trying to load and use a previously trained agent – it will also allow you to resume training after it has been interrupted, etc.\n",
    "\n",
    "Let's start by creating a simple SAC agent from a preset and training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = sac_simple(\n",
    "    'Pendulum-v0', stop_criterion=-250, seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:17, 57.03it/s, alpha=0.753, env_step=1000, len=200, loss/actor=25.850, loss/alpha=-0.651, loss/critic1=0.384, loss/critic2=0.406, n/ep=0, n/st=1, rew=-1829.07]                          \n",
      "Epoch #2:   1%|1         | 11/1000 [00:00<00:14, 70.18it/s, alpha=0.751, env_step=1011, len=200, loss/actor=26.207, loss/alpha=-0.657, loss/critic1=0.384, loss/critic2=0.406, n/ep=0, n/st=1, rew=-1829.07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -1810.144515 ± 148.657324, best_reward: -1186.824446 ± 304.551268 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:16, 60.63it/s, alpha=0.565, env_step=2000, len=200, loss/actor=55.363, loss/alpha=-1.139, loss/critic1=0.530, loss/critic2=0.707, n/ep=0, n/st=1, rew=-1547.19]                          \n",
      "Epoch #3:   1%|1         | 10/1000 [00:00<00:14, 67.96it/s, alpha=0.564, env_step=2010, len=200, loss/actor=55.721, loss/alpha=-1.141, loss/critic1=0.531, loss/critic2=0.710, n/ep=0, n/st=1, rew=-1547.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -1135.350921 ± 102.566455, best_reward: -1135.350921 ± 102.566455 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 1001it [00:16, 59.15it/s, alpha=0.442, env_step=3000, len=200, loss/actor=74.259, loss/alpha=-1.162, loss/critic1=0.816, loss/critic2=1.078, n/ep=0, n/st=1, rew=-387.59]                          \n",
      "Epoch #4:   1%|1         | 10/1000 [00:00<00:14, 66.67it/s, alpha=0.441, env_step=3010, len=200, loss/actor=74.476, loss/alpha=-1.161, loss/critic1=0.869, loss/critic2=1.133, n/ep=0, n/st=1, rew=-387.59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -1003.528745 ± 601.529164, best_reward: -1003.528745 ± 601.529164 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 1001it [00:42, 23.72it/s, alpha=0.352, env_step=4000, len=200, loss/actor=82.729, loss/alpha=-1.147, loss/critic1=1.822, loss/critic2=1.651, n/ep=0, n/st=1, rew=-1.53]                          \n",
      "Epoch #5:   1%|1         | 11/1000 [00:00<00:14, 69.57it/s, alpha=0.351, env_step=4011, len=200, loss/actor=82.332, loss/alpha=-1.164, loss/critic1=1.802, loss/critic2=1.626, n/ep=0, n/st=1, rew=-1.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -373.723060 ± 335.072812, best_reward: -373.723060 ± 335.072812 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 1001it [00:28, 35.11it/s, alpha=0.279, env_step=5000, len=200, loss/actor=88.286, loss/alpha=-1.286, loss/critic1=2.950, loss/critic2=2.810, n/ep=0, n/st=1, rew=-1355.56]                          \n",
      "Epoch #6:   1%|1         | 13/1000 [00:00<00:12, 79.21it/s, alpha=0.278, env_step=5013, len=200, loss/actor=88.141, loss/alpha=-1.292, loss/critic1=2.938, loss/critic2=2.859, n/ep=0, n/st=1, rew=-1355.56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -427.622719 ± 461.578917, best_reward: -373.723060 ± 335.072812 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6:  47%|####7     | 472/1000 [00:46<00:51, 10.19it/s, env_step=5472, len=200, n/ep=1, n/st=1, rew=-123.81]\n"
     ]
    }
   ],
   "source": [
    "train_results = agent.train(max_epoch=10, step_per_epoch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at what the training progress looks like in terms of epoch, environment steps and learning steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5471, 5471)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.epoch, agent.env_step, agent.gradient_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading an Agent from a Checkpoint\n",
    "\n",
    "By default, the agent is checkpointed every time there is an improvement. The checkpointed state goes under the logging path and we can load it back using ``torch.load``. It has the form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"log/Pendulum-v0/sac/best_agent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the checkpointed state, we can write it back into a new agent of the same kind using ``agent.load_state_dict``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = sac_simple(\n",
    "    'Pendulum-v0', stop_criterion=-250, seed=0\n",
    ")\n",
    "\n",
    "agent.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe the epoch, environment step and the gradient step of this new agent now, you will see that it displays the training that we did in our previous agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5471, 5471)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.epoch, agent.env_step, agent.gradient_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run ``agent.test`` and observe the performance of the agent to verify that the trained policy was restored as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 2000,\n",
       " 'rews': array([-126.8196232 , -118.639652  , -126.15630426, -123.37939039,\n",
       "        -127.0365974 , -246.65841828, -129.50245252, -240.11327871,\n",
       "        -127.65953754, -126.30450594]),\n",
       " 'lens': array([200, 200, 200, 200, 200, 200, 200, 200, 200, 200]),\n",
       " 'idxs': array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Agent Manually\n",
    "\n",
    "You can also save the state of the agent manually – the interface is similar to that used in PyTorch. You retrieve the state of the agent using ``agent.state_dict()`` and then save it using ``torch.save``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = agent.state_dict()\n",
    "torch.save(agent, \"output/agent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the state of the agent back, you can then construct a new agent, load the state dict using ``torch.load`` and fill it into the agent using ``agent.load_state_dict``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = sac_simple(\n",
    "    'Pendulum-v0', stop_criterion=-250, seed=0\n",
    ")\n",
    "\n",
    "state_dict = torch.load(\"output/agent.pth\")\n",
    "agent2.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stats regarding training progress will again be retained as well as the saved policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b08d363ebb8492a302c7076da18bf168d910622d9da13f07c6e53914cde27110"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
